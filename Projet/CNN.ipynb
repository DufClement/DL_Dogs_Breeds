{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "private-particular",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.image import imread\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "municipal-pacific",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dogs_breed\\train\n"
     ]
    }
   ],
   "source": [
    "base_dir = 'dogs_breed'    \n",
    "valid_dir = 'valid'\n",
    "train_dir = 'train'\n",
    "train_path = os.path.join(base_dir, train_dir)\n",
    "valid_path = os.path.join(base_dir, valid_dir)\n",
    "print(train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "motivated-audience",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    " rescale=1./255,rotation_range=40,\n",
    " width_shift_range=0.2,\n",
    " height_shift_range=0.2,\n",
    " shear_range=0.2,\n",
    " zoom_range=0.2,\n",
    " horizontal_flip=True)\n",
    "\n",
    "val_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "processed-digit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16469 images belonging to 120 classes.\n",
      "dogs_breed\\train\n"
     ]
    }
   ],
   "source": [
    "train_gen = train_datagen.flow_from_directory(\n",
    "    train_path,\n",
    "    target_size=(150, 150),\n",
    "    batch_size=20,\n",
    "    class_mode='binary'\n",
    ")\n",
    "print(train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "orange-heath",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4111 images belonging to 120 classes.\n"
     ]
    }
   ],
   "source": [
    "val_gen = val_datagen.flow_from_directory(\n",
    "    valid_path,\n",
    "    target_size=(150, 150),\n",
    "    batch_size=20,\n",
    "    class_mode='binary'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "allied-houston",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "directed-vegetation",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 150, 150, 3)]     0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 148, 148, 16)      448       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 74, 74, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 72, 72, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 36, 36, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 34, 34, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 17, 17, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 18496)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 512)               9470464   \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 120)               61560     \n",
      "=================================================================\n",
      "Total params: 9,555,608\n",
      "Trainable params: 9,555,608\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "def create_model(use_dropout :bool = False):\n",
    "  img_input = layers.Input(shape=(150, 150, 3))\n",
    "  x = layers.Conv2D(16, 3, activation='relu')(img_input)\n",
    "  x = layers.MaxPooling2D(2)(x)\n",
    "  x = layers.Conv2D(32, 3, activation='relu')(x)\n",
    "  x = layers.MaxPooling2D(2)(x)\n",
    "  x = layers.Conv2D(64, 3, activation='relu')(x)\n",
    "  x = layers.MaxPooling2D(2)(x)\n",
    "  x = layers.Flatten()(x)\n",
    "  x = layers.Dense(512, activation='relu')(x)\n",
    "\n",
    "  if use_dropout:\n",
    "      x = layers.Dropout(0.5)(x)\n",
    "\n",
    "  # Create output layer\n",
    "  output = layers.Dense(120, activation='softmax')(x) # species number\n",
    "\n",
    "\n",
    "  if 'model' in locals():\n",
    "      del model \n",
    "\n",
    "  model_intra = Model(img_input, output)\n",
    "\n",
    "  model_intra.summary()\n",
    "\n",
    "  model_intra.compile(\n",
    "      loss='categorical_crossentropy',\n",
    "      optimizer=RMSprop(lr=0.001),\n",
    "      metrics=['acc']\n",
    "  )\n",
    "  return model_intra\n",
    "\n",
    "model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "sorted-custody",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16469 images belonging to 120 classes.\n",
      "Found 4111 images belonging to 120 classes.\n",
      "Epoch 1/15\n",
      "823/823 - 127s - loss: 4.6013 - acc: 0.0287 - val_loss: 4.3931 - val_acc: 0.0507\n",
      "Epoch 2/15\n",
      "823/823 - 122s - loss: 4.0510 - acc: 0.0978 - val_loss: 4.0159 - val_acc: 0.0968\n",
      "Epoch 3/15\n",
      "823/823 - 121s - loss: 3.3628 - acc: 0.2141 - val_loss: 4.0485 - val_acc: 0.1112\n",
      "Epoch 4/15\n",
      "823/823 - 122s - loss: 2.1647 - acc: 0.4586 - val_loss: 5.0169 - val_acc: 0.1100\n",
      "Epoch 5/15\n",
      "823/823 - 124s - loss: 0.9175 - acc: 0.7528 - val_loss: 7.0064 - val_acc: 0.0890\n",
      "Epoch 6/15\n",
      "823/823 - 121s - loss: 0.3125 - acc: 0.9183 - val_loss: 10.0854 - val_acc: 0.0915\n",
      "Epoch 7/15\n",
      "823/823 - 124s - loss: 0.1826 - acc: 0.9531 - val_loss: 11.7869 - val_acc: 0.0907\n",
      "Epoch 8/15\n",
      "823/823 - 128s - loss: 0.1477 - acc: 0.9652 - val_loss: 12.9408 - val_acc: 0.0844\n",
      "Epoch 9/15\n",
      "823/823 - 129s - loss: 0.1332 - acc: 0.9700 - val_loss: 13.2082 - val_acc: 0.0905\n",
      "Epoch 10/15\n",
      "823/823 - 124s - loss: 0.1091 - acc: 0.9768 - val_loss: 13.7186 - val_acc: 0.0890\n",
      "Epoch 11/15\n",
      "823/823 - 120s - loss: 0.1062 - acc: 0.9785 - val_loss: 14.7849 - val_acc: 0.0907\n",
      "Epoch 12/15\n",
      "823/823 - 120s - loss: 0.0998 - acc: 0.9801 - val_loss: 12.9734 - val_acc: 0.0859\n",
      "Epoch 13/15\n",
      "823/823 - 120s - loss: 0.0952 - acc: 0.9830 - val_loss: 15.9641 - val_acc: 0.0790\n",
      "Epoch 14/15\n",
      "823/823 - 120s - loss: 0.0878 - acc: 0.9829 - val_loss: 14.6004 - val_acc: 0.0907\n",
      "Epoch 15/15\n",
      "823/823 - 121s - loss: 0.0876 - acc: 0.9835 - val_loss: 16.8187 - val_acc: 0.0849\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "def train_model(\n",
    "    train_img_nb :int, \n",
    "    valid_img_nb :int, \n",
    "    batch_size :int,\n",
    "    epochs :int, \n",
    "    use_data_augmentation :bool = False\n",
    "):\n",
    "    # All images will be rescaled by 1./255\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True\n",
    "    ) if use_data_augmentation else ImageDataGenerator(\n",
    "        rescale=1./255\n",
    "    )\n",
    "    \n",
    "    val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "    \n",
    "    train_gen = train_datagen.flow_from_directory(\n",
    "        train_path,\n",
    "        target_size=(150, 150),\n",
    "        batch_size=20,\n",
    "        classes=os.listdir(train_path)\n",
    "    )\n",
    "    \n",
    "    valid_generator = val_datagen.flow_from_directory(\n",
    "        valid_path,\n",
    "        target_size=(150, 150),\n",
    "        batch_size=20,\n",
    "        classes=os.listdir(valid_path)\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        train_gen,\n",
    "        steps_per_epoch=int(train_img_nb / batch_size),\n",
    "        epochs=epochs,\n",
    "        validation_data=valid_generator,\n",
    "        validation_steps=int(valid_img_nb / batch_size),\n",
    "        verbose=2\n",
    "    )\n",
    "    return history\n",
    "\n",
    "history = train_model(\n",
    "    16469, \n",
    "    4111, \n",
    "    batch_size=20,\n",
    "    epochs=15\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conditional-manhattan",
   "metadata": {},
   "source": [
    "# Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "durable-surface",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, Model\n",
    "\n",
    "def model_create_augmentation():\n",
    "    inputs = layers.Input(shape=(150, 150, 3))\n",
    "    x = layers.Conv2D(16, 3, activation='relu')(inputs)\n",
    "    x = layers.MaxPooling2D(2)(x)\n",
    "    x = layers.Conv2D(32, 3, activation='relu')(x)\n",
    "    x = layers.MaxPooling2D(2)(x)\n",
    "    x = layers.Conv2D(64, 3, activation='relu')(x)\n",
    "    x = layers.MaxPooling2D(2)(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(512, activation='relu')(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "\n",
    "    outputs = layers.Dense(120, activation='softmax')(x) # species number \n",
    "\n",
    "    m = Model(img_input, output)\n",
    "\n",
    "    m.summary()\n",
    "\n",
    "    m.compile(loss='categorical_crossentropy',optimizer=['rmsprop'],metrics=['acc'])\n",
    "\n",
    "    return m\n",
    "\n",
    "model = model_create_augmentation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intended-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "def train_model_augmentation():\n",
    "    train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "    val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "    train_gen = train_datagen.flow_from_directory(\n",
    "        train_path,\n",
    "        target_size=(150, 150),\n",
    "        batch_size=20,\n",
    "        classes=os.listdir(train_path)\n",
    "    )\n",
    "\n",
    "    valid_gen = val_datagen.flow_from_directory(\n",
    "        valid_path,\n",
    "        target_size=(150, 150),\n",
    "        batch_size=20,\n",
    "        classes=os.listdir(valid_path)\n",
    "    )\n",
    "\n",
    "    algo = model.fit(\n",
    "        train_gen,\n",
    "        steps_per_epoch=int(16469 / 20),\n",
    "        epochs=15,\n",
    "        validation_data=valid_gen,\n",
    "        validation_steps=int(4111 / 20)\n",
    "    )\n",
    "    return algo\n",
    "\n",
    "algo = train_model_augmentation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "gross-justice",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 150, 150, 3)]     0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 148, 148, 16)      448       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 74, 74, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 72, 72, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 36, 36, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 34, 34, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 17, 17, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 18496)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               9470464   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 120)               61560     \n",
      "=================================================================\n",
      "Total params: 9,555,608\n",
      "Trainable params: 9,555,608\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Found 16469 images belonging to 120 classes.\n",
      "Found 4111 images belonging to 120 classes.\n",
      "Epoch 1/15\n",
      "823/823 - 141s - loss: 4.7861 - acc: 0.0142 - val_loss: 4.6457 - val_acc: 0.0241\n",
      "Epoch 2/15\n",
      "823/823 - 142s - loss: 4.6214 - acc: 0.0243 - val_loss: 4.3765 - val_acc: 0.0478\n",
      "Epoch 3/15\n",
      "823/823 - 140s - loss: 4.4891 - acc: 0.0361 - val_loss: 4.2682 - val_acc: 0.0593\n",
      "Epoch 4/15\n",
      "823/823 - 143s - loss: 4.4095 - acc: 0.0416 - val_loss: 4.2432 - val_acc: 0.0568\n",
      "Epoch 5/15\n",
      "823/823 - 141s - loss: 4.3414 - acc: 0.0512 - val_loss: 4.2571 - val_acc: 0.0507\n",
      "Epoch 6/15\n",
      "823/823 - 143s - loss: 4.3034 - acc: 0.0593 - val_loss: 4.1545 - val_acc: 0.0673\n",
      "Epoch 7/15\n",
      "823/823 - 144s - loss: 4.2681 - acc: 0.0607 - val_loss: 4.0705 - val_acc: 0.0820\n",
      "Epoch 8/15\n",
      "823/823 - 143s - loss: 4.2535 - acc: 0.0642 - val_loss: 4.0565 - val_acc: 0.0851\n",
      "Epoch 9/15\n",
      "823/823 - 145s - loss: 4.2424 - acc: 0.0646 - val_loss: 4.0550 - val_acc: 0.0832\n",
      "Epoch 10/15\n",
      "823/823 - 142s - loss: 4.2329 - acc: 0.0660 - val_loss: 4.0541 - val_acc: 0.0915\n",
      "Epoch 11/15\n",
      "823/823 - 143s - loss: 4.2281 - acc: 0.0644 - val_loss: 4.0799 - val_acc: 0.0849\n",
      "Epoch 12/15\n",
      "823/823 - 142s - loss: 4.2228 - acc: 0.0668 - val_loss: 4.1074 - val_acc: 0.0729\n",
      "Epoch 13/15\n",
      "823/823 - 139s - loss: 4.2249 - acc: 0.0677 - val_loss: 4.1166 - val_acc: 0.0805\n",
      "Epoch 14/15\n",
      "823/823 - 140s - loss: 4.2192 - acc: 0.0682 - val_loss: 4.0897 - val_acc: 0.0810\n",
      "Epoch 15/15\n",
      "823/823 - 140s - loss: 4.2213 - acc: 0.0656 - val_loss: 4.1020 - val_acc: 0.0732\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satisfied-rehabilitation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "herbal-marks",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
